{
  "rules": [
    {
      "id": "RL001",
      "category": "Structure",
      "description": "Check for proper bronze, silver, gold layer structure in the Lakehouse.",
      "pyspark_query": "# Check lakehouse layer structure\nlayer = None\nif any(x in lakehouse_name.lower() for x in ['bronze', 'raw', 'landing']):\n    layer = 'bronze'\nelif any(x in lakehouse_name.lower() for x in ['silver', 'refined', 'intermediate']):\n    layer = 'silver'\nelif any(x in lakehouse_name.lower() for x in ['gold', 'curated', 'consumption']):\n    layer = 'gold'\n\nif layer:\n    print(f\"Table appears to be in the {layer} layer based on lakehouse name.\")\nelse:\n    print(f\"Lakehouse name does not follow the bronze/silver/gold naming pattern: {lakehouse_name}\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if layer else \"Anomaly - ERR_1001\"))",
      "recommendation": "Organize your Lakehouse data into logical layers (bronze/raw, silver/refined, gold/consumption) to maintain clean data architecture.",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "lakehouse"
    },
    {
      "id": "RL002",
      "category": "Structure",
      "description": "Check if tables are properly partitioned for efficient querying.",
      "pyspark_query": "# Analyze partitioning from delta log\n# Look for partition column information in delta log\npartition_columns = None\nfor _, row in df_all.iterrows():\n    if 'metadata_partitionColumns' in row:\n        try:\n            if isinstance(row['metadata_partitionColumns'], str):\n                partition_columns = json.loads(row['metadata_partitionColumns'].replace(\"'\", '\"'))\n                break\n            elif isinstance(row['metadata_partitionColumns'], list):\n                partition_columns = row['metadata_partitionColumns']\n                break\n        except Exception as e:\n            pass\n\n# Count files to understand partitioning impact\nfile_paths = set()\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if col.startswith('add_') and '_path' in col and isinstance(row[col], str):\n            file_paths.add(row[col])\n\nnum_files = len(file_paths)\n\n# Report findings\nif partition_columns and len(partition_columns) > 0:\n    print(f\"Table is partitioned by: {', '.join(partition_columns)}\")\n    print(f\"Total data files: {num_files}\")\n    \n    if num_files > 1000:\n        print(\"WARNING: Very high file count. Current partitioning may be too granular.\")\n        print(\"Consider using fewer or coarser partition columns\")\n    elif num_files < 5 and num_files > 0:\n        print(\"WARNING: Very few files despite partitioning. Partitioning may not be beneficial.\")\n        print(\"Consider if partitioning is needed for this data volume\")\nelse:\n    print(\"Table is not partitioned\")\n    \n    if num_files > 100:\n        print(f\"WARNING: Table has many files ({num_files}) without partitioning\")\n        print(\"Consider partitioning by frequently filtered columns (date, region, etc.)\")\n    else:\n        print(f\"Non-partitioned table with {num_files} files is reasonable for current size\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if ((not partition_columns and num_files > 100) or (partition_columns and num_files < 5)) else \"Optimized - OPT_2001\"))",
      "recommendation": "Partition large tables (>100GB) by frequently filtered columns like date or region. Over-partitioning small tables can reduce performance.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL003",
      "category": "Structure",
      "description": "Check for Z-order indexing on tables that would benefit from it.",
      "pyspark_query": "# Analyze Z-order indexing from delta log\n# Check if Z-Order has been applied based on commit history\nzorder_operations = []\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'commitInfo' in col and isinstance(row[col], str) and 'operation' in row[col]:\n            try:\n                commit_info = json.loads(row[col].replace(\"'\", '\"'))\n                if 'zOrderBy' in row[col] or (commit_info.get('operation') == 'OPTIMIZE' and 'zOrderBy' in row[col]):\n                    if 'timestamp' in commit_info:\n                        zorder_operations.append({\n                            'timestamp': commit_info.get('timestamp'),\n                            'version': row.get('log_version'),\n                            'columns': commit_info.get('zOrderBy', [])\n                        })\n                    else:\n                        zorder_operations.append({\n                            'version': row.get('log_version'),\n                            'columns': []\n                        })\n            except Exception as e:\n                pass\n\n# Estimate table size to determine if Z-order would be beneficial\ntotal_size_bytes = 0\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if col.startswith('add_') and '_size' in col:\n            try:\n                size = float(row[col])\n                total_size_bytes += size\n            except:\n                pass\n\n# Report findings\nif zorder_operations:\n    print(f\"Z-Order indexing has been applied {len(zorder_operations)} times\")\n    if any('columns' in op and op['columns'] for op in zorder_operations):\n        ordered_columns = next((op['columns'] for op in zorder_operations if 'columns' in op and op['columns']), [])\n        print(f\"Z-Ordered by columns: {', '.join(ordered_columns) if ordered_columns else 'unknown'}\")\n    \n    latest_op = max(zorder_operations, key=lambda x: x.get('timestamp', 0) if 'timestamp' in x else 0)\n    if 'timestamp' in latest_op:\n        last_zorder_time = datetime.fromtimestamp(latest_op['timestamp']/1000).strftime('%Y-%m-%d')\n        print(f\"Last Z-Order operation: {last_zorder_time}\")\nelse:\n    print(\"No Z-Order indexing operations found in delta log history\")\n    \n    # Convert to GB to check if table is large enough to benefit from Z-Order\n    total_size_gb = total_size_bytes / (1024 * 1024 * 1024)\n    if total_size_gb > 10:\n        print(f\"WARNING: Large table size (~{total_size_gb:.1f} GB) would benefit from Z-Order indexing\")\n        print(\"Consider OPTIMIZE with ZORDER BY for frequently filtered/joined columns\")\n        \n        # Check schema to suggest candidate columns\n        schema_info = None\n        for _, row in df_all.iterrows():\n            for col in row.index:\n                if 'metadata_schema' in col and isinstance(row[col], str):\n                    try:\n                        schema = json.loads(row[col].replace(\"'\", '\"'))\n                        if 'fields' in schema:\n                            schema_info = schema\n                            break\n                    except:\n                        pass\n            if schema_info:\n                break\n        if schema_info and 'fields' in schema_info:\n            candidate_columns = []\n            for field in schema_info['fields']:\n                if 'name' not in field:\n                    continue\n                name = field['name'].lower()\n                if (name.endswith('id') or name.endswith('key') or name.endswith('date') or 'date' in name or name.endswith('code') or any(dim in name for dim in ['region', 'country', 'state', 'city', 'category', 'type'])):\n                    candidate_columns.append(field['name'])\n            if candidate_columns:\n                print(f\"Candidate columns for Z-Order: {', '.join(candidate_columns[:3])}{'...' if len(candidate_columns) > 3 else ''}\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if (zorder_operations or (total_size_bytes/(1024*1024*1024) <= 10)) else \"Anomaly - ERR_1001\"))",
      "recommendation": "Use Z-order indexing (OPTIMIZE table ZORDER BY columns) on large tables for frequently filtered or join columns to improve query performance.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL004",
      "category": "Schema",
      "description": "Check for proper column naming patterns.",
      "pyspark_query": "# Analyze schema for column naming patterns\nimport re\n\n# Extract schema from delta log\nschema_info = None\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\nissues_detected = False\nif schema_info and 'fields' in schema_info:\n    naming_issues = []\n    total_fields = len(schema_info['fields'])\n    \n    for field in schema_info['fields']:\n        if 'name' in field:\n            col_name = field['name']\n            \n            # Check naming convention\n            if ' ' in col_name:\n                naming_issues.append(f\"Column '{col_name}' contains spaces\")\n                issues_detected = True\n            elif not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', col_name):\n                naming_issues.append(f\"Column '{col_name}' doesn't follow standard naming pattern\")\n                issues_detected = True\n            elif col_name.isupper() or col_name.islower():\n                # This is just informational, not necessarily an issue\n                pass\n    \n    if naming_issues:\n        print(f\"Found {len(naming_issues)} naming issues in {total_fields} columns:\")\n        for issue in naming_issues[:5]:\n            print(f\"    {issue}\")\n        if len(naming_issues) > 5:\n            print(f\"    ...and {len(naming_issues) - 5} more naming issues\")\n    else:\n        print(f\"All {total_fields} columns follow good naming conventions\")\nelse:\n    print(\"Could not find schema information in delta log\")\n    issues_detected = True  # Mark as issue if we can't find schema info\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issues_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Use consistent naming patterns (snake_case or camelCase). Avoid spaces in names and special characters except underscore.",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL005",
      "category": "Schema",
      "description": "Check for missing nullability constraints on primary key fields.",
      "pyspark_query": "# Analyze the delta log for schema information\nkey_columns = []\nschema_info = None\nnullable_keys_found = False\n\n# Find the latest schema information in the delta log\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_data = json.loads(row[col].replace(\"'\", '\"'))\n                if 'fields' in schema_data:\n                    schema_info = schema_data\n                    break\n            except:\n                pass\n    if schema_info:\n        break\n\nif schema_info and 'fields' in schema_info:\n    for field in schema_info['fields']:\n        if 'name' in field:\n            field_name = field['name']\n            is_nullable = field.get('nullable', True)  # Default to True if not specified\n            field_type = field.get('type', 'unknown')\n            \n            # Identify primary key columns\n            if field_name.lower().endswith('id') or field_name.lower() == 'key' or 'key' in field_name.lower() or field_name.lower() == 'pk':\n                key_columns.append(field_name)\n                if is_nullable:\n                    print(f\"WARNING: Key column '{field_name}' ({field_type}) allows NULL values\")\n                    nullable_keys_found = True\n                else:\n                    print(f\"Key column '{field_name}' ({field_type}) properly set as NOT NULL\")\n    \n    if not key_columns:\n        print(\"No obvious primary key columns detected based on column naming patterns\")\nelse:\n    print(f\"No detailed schema information available for {table_name}\")\n    \n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if nullable_keys_found else \"Optimized - OPT_2001\"))",
      "recommendation": "Ensure primary key columns (IDs, join fields) don't allow nulls. Use NOT NULL constraints where appropriate in table definitions.",
      "status": "true",
      "severity": 1,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL006",
      "category": "Maintenance",
      "description": "Check for time travel retention policies.",
      "pyspark_query": "# Check delta log version count and retention settings\nversion_count = len(df_all)\n\nprint(f\"Total versions in log: {version_count}\")\n\nif version_count > 100:\n    print(f\"WARNING: Large history ({version_count} versions). Consider setting a retention policy.\")\n\n# Look for retention settings in metadata\nretention_duration = None\ndeleted_file_retention = None\n\nfor _, row in df_all.iterrows():\n    if 'metadata_configuration' in row:\n        try:\n            if isinstance(row['metadata_configuration'], str):\n                config = json.loads(row['metadata_configuration'].replace(\"'\", '\"'))\n                retention_duration = config.get('logRetentionDuration')\n                deleted_file_retention = config.get('deletedFileRetentionDuration')\n                break\n        except:\n            pass\n\nif retention_duration:\n    print(f\"Log retention duration: {retention_duration}\")\nelse:\n    print(f\"No log retention duration configured. Consider setting 'delta.logRetentionDuration'\")\n    \nif deleted_file_retention:\n    print(f\"Deleted file retention: {deleted_file_retention}\")\nelse:\n    print(f\"No deleted file retention configured. Consider setting 'delta.deletedFileRetentionDuration'\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if retention_duration and deleted_file_retention else \"Anomaly - ERR_1001\"))",
      "recommendation": "Set appropriate retention periods for time travel based on your needs. Balance history requirements with storage costs.",
      "remediation_template": "ALTER TABLE {full_table_name} SET TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 30 days', 'delta.deletedFileRetentionDuration' = 'interval 7 days')",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL007",
      "category": "Compression",
      "description": "Check for compression efficiency in table data.",
      "pyspark_query": "# Analyze compression efficiency from delta log\n# Look for compression information\nnum_rows = 0\nfile_size_bytes = 0\nnum_files = 0\n\n# Get size and stats information\nfor _, row in df_all.iterrows():\n    # Collect file sizes\n    for col in row.index:\n        if col.startswith(\"add_\") and \"_size\" in col:\n            try:\n                size = float(row[col])\n                file_size_bytes += size\n                num_files += 1\n            except:\n                pass\n        \n        # Try to get row count from stats\n        if col.startswith('add_') and '_stats' in col and isinstance(row[col], str):\n            try:\n                stats = json.loads(row[col].replace(\"'\", '\"'))\n                if 'numRecords' in stats:\n                    num_rows += int(stats['numRecords'])\n            except:\n                pass\n\n# Calculate average row size and report on compression efficiency\nif num_rows > 0 and file_size_bytes > 0:\n    avg_row_size_bytes = file_size_bytes / num_rows\n    avg_file_size_mb = file_size_bytes / (1024 * 1024) / max(1, num_files)\n    \n    print(f\"Average row size: {round(avg_row_size_bytes, 2)} bytes\")\n    print(f\"Average file size: {round(avg_file_size_mb, 2)} MB per file\")\n    print(f\"Total data size: {round(file_size_bytes / (1024 * 1024), 2)} MB\")\n    print(f\"Row count: {num_rows:,}\")\n    \n    if avg_row_size_bytes > 10000:\n        print(\"WARNING: Average row size is very high (>10KB)\")\n        print(\"Consider reviewing column data types and compression settings\")\n        print(\"Large string columns might benefit from separate storage\")\n        issue_detected = True\n    else:\n        issue_detected = False\n\n    if avg_file_size_mb < 25:\n        print(\"WARNING: Average file size is small (<25MB)\")\n        print(\"Consider running OPTIMIZE to combine small files\")\n        issue_detected = True\n    elif avg_file_size_mb > 1000:\n        print(\"WARNING: Average file size is very large (>1GB)\")\n        print(\"Consider more granular partitioning or splitting large files\")\n        issue_detected = True\n    else:\n        issue_detected = issue_detected or False\nelse:\n    print(\"Could not determine compression efficiency - insufficient statistics in delta log\")\n    issue_detected = True\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Aim for optimal row size and compressed file size. Monitor compression ratios and optimize data types for better efficiency.",
      "remediation_template": "OPTIMIZE {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL008",
      "category": "Schema",
      "description": "Check for schema evolution compatibility settings.",
      "pyspark_query": "# Check for schema evolution settings\n# Look for schema evolution settings in metadata\nhas_merge_schema_setting = False\nschema_mode = None\n\nfor _, row in df_all.iterrows():\n    if 'metadata_configuration' in row:\n        try:\n            if isinstance(row['metadata_configuration'], str):\n                config = json.loads(row['metadata_configuration'].replace(\"'\", '\"'))\n                if 'mergeSchema' in str(config):\n                    has_merge_schema_setting = True\n                if 'schemaMode' in config:\n                    schema_mode = config['schemaMode']\n                break\n        except:\n            pass\n\nif has_merge_schema_setting:\n    print(\"mergeSchema setting is enabled (good for schema evolution)\")\nelse:\n    print(\"mergeSchema setting not found. Consider enabling for compatible schema changes.\")\n    \nif schema_mode:\n    print(f\"Schema mode is set to: {schema_mode}\")\n    if schema_mode.lower() == 'enforceread':\n        print(\"Strict schema enforcement is active. This may cause failures if schema changes.\")\nelse:\n    print(\"No specific schema mode configured\")\n    \nprint(\"Recommendation: For tables with evolving schemas, enable mergeSchema and consider schema versioning for major changes.\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if has_merge_schema_setting else \"Anomaly - ERR_1001\"))",
      "recommendation": "Configure schema evolution settings appropriately. Enable 'mergeSchema' for compatible changes. For major schema changes, create a new table version.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL009",
      "category": "Schema",
      "description": "Check for proper data types in columns.",
      "pyspark_query": "# Analyze schema for data type patterns\n# Extract schema from delta log\nschema_info = None\ndatatype_issues = []\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\nif schema_info and 'fields' in schema_info:\n    datatype_issues = []\n    total_fields = len(schema_info['fields'])\n    \n    for field in schema_info['fields']:\n        if 'name' in field and 'type' in field:\n            col_name = field['name']\n            data_type = str(field['type'])\n            \n            # Check for overly generic string lengths\n            if isinstance(data_type, str):\n                if 'varchar' in data_type.lower() and '(max)' in data_type.lower():\n                    datatype_issues.append(f\"Column '{col_name}' uses generic varchar(max)\")\n                \n                # Check for obsolete types\n                if data_type.lower() in ['text', 'ntext', 'image']:\n                    datatype_issues.append(f\"Column '{col_name}' uses obsolete data type {data_type}\")\n                    \n                # Check for excessive precision in numeric types\n                if 'decimal' in data_type.lower() and ',38)' in data_type.lower():\n                    datatype_issues.append(f\"Column '{col_name}' may have excessive precision: {data_type}\")\n    \n    if datatype_issues:\n        print(f\"Found {len(datatype_issues)} data type issues in {total_fields} columns:\")\n        for issue in datatype_issues[:5]:\n            print(f\"    {issue}\")\n        if len(datatype_issues) > 5:\n            print(f\"    ...and {len(datatype_issues) - 5} more data type issues\")\n    else:\n        print(f\"All {total_fields} columns use appropriate data types\")\nelse:\n    print(\"Could not find schema information in delta log\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if datatype_issues else \"Optimized - OPT_2001\"))",
      "recommendation": "Use specific data types with appropriate lengths. Avoid obsolete types and excessive precision.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL010",
      "category": "Metadata",
      "description": "Check column-level documentation.",
      "pyspark_query": "# Check for column documentation in delta log\n\nschema_info = None\ncolumn_comments = 0\ntotal_columns = 0\ndocumented_pct = 0\n\n# Extract schema with comments\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema = json.loads(row[col].replace(\"'\", '\"'))\n                if 'fields' in schema:\n                    schema_info = schema\n                    total_columns = len(schema['fields'])\n                    \n                    for field in schema['fields']:\n                        if 'metadata' in field and field['metadata'] and 'comment' in field['metadata']:\n                            column_comments += 1\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\n# Report on column documentation\nif total_columns > 0:\n    documented_pct = (column_comments / total_columns) * 100\n    print(f\"{column_comments} of {total_columns} columns have documentation ({documented_pct:.1f}%)\")\n    \n    if documented_pct < 50:\n        print(\"FINDING: Less than 50% of columns are documented\")\n        \n        # If we have schema info, list some undocumented columns\n        if schema_info:\n            undocumented = []\n            for field in schema_info['fields'][:10]:  # Check first 10 fields only\n                has_comment = False\n                if 'metadata' in field and field['metadata'] and 'comment' in field['metadata']:\n                    has_comment = True\n                    \n                if not has_comment and 'name' in field:\n                    undocumented.append(field['name'])\n                    \n            if undocumented:\n                print(f\"Undocumented columns include: {', '.join(undocumented[:5])}{'...' if len(undocumented) > 5 else ''}\")\nelse:\n    print(\"Could not find schema information to check column documentation\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if documented_pct < 50 else \"Optimized - OPT_2001\"))",
      "recommendation": "Document every column with clear descriptions of data meaning, format, and business context.",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL011",
      "category": "Performance",
      "description": "Check for data distribution skew in key columns.",
      "pyspark_query": "# Analyze data distribution skew from stats\n# Look for column statistics that might indicate data skew\ncolumn_stats = {}\ntotal_rows = 0\npotential_skew = []\n\n# Get stats from delta log\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'add_' in col and '_stats' in col and isinstance(row[col], str):\n            try:\n                stats = json.loads(row[col].replace(\"'\", '\"'))\n                if 'numRecords' in stats:\n                    records = stats['numRecords']\n                    if records > total_rows:\n                        total_rows = records\n                if 'stats' in stats:\n                    col_stats = stats.get('stats', {})\n                    \n                    # Look for columns with very high max:min ratios or high distinct count ratios\n                    for column_name, col_stat in col_stats.items():\n                        if 'distinctCount' in col_stat and 'min' in col_stat and 'max' in col_stat:\n                            distinct_count = col_stat['distinctCount']\n                            # If distinctCount is very low compared to rows, potential skew\n                            if distinct_count > 0 and total_rows / distinct_count > 1000:\n                                potential_skew.append((column_name, distinct_count, total_rows / distinct_count))\n            except:\n                pass\n\n# Also check partition columns for potential skew\npartition_columns = []\nfor _, row in df_all.iterrows():\n    if 'metadata_partitionColumns' in row:\n        try:\n            if isinstance(row['metadata_partitionColumns'], str):\n                partition_cols = json.loads(row['metadata_partitionColumns'].replace(\"'\", '\"'))\n                if partition_cols:\n                    partition_columns = partition_cols\n                    break\n            elif isinstance(row['metadata_partitionColumns'], list):\n                partition_columns = row['metadata_partitionColumns']\n                break\n        except Exception as e:\n            pass\n\nif total_rows > 0:\n    if potential_skew:\n        print(f\"Potential data skew detected in {len(potential_skew)} columns:\")\n        for col, distinct, ratio in sorted(potential_skew, key=lambda x: x[2], reverse=True)[:3]:\n            print(f\"    Column '{col}': {distinct} distinct values, potential skew ratio: {ratio:.1f}x\")\n        print(\"Consider examining data distribution and adjusting partitioning if needed\")\n    else:\n        print(\"No obvious data skew detected in available statistics\")\n        \n    if partition_columns:\n        print(f\"Table is partitioned by: {', '.join(partition_columns)}\")\n        print(\"Check partition sizes to ensure even distribution\")\nelse:\n    print(\"Insufficient statistics to analyze data distribution skew\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if potential_skew else \"Optimized - OPT_2001\"))",
      "recommendation": "Address data skew by adjusting partitioning strategy, adding salting techniques for hotspot keys, or implementing more uniform distributions.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL012",
      "category": "Performance",
      "description": "Check for column encoding efficiency.",
      "pyspark_query": "# Analyze column encoding efficiency\n# Extract schema to identify columns that could benefit from specific encodings\nschema_info = None\ncolumn_types = {}\n\n# Find schema information\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                if 'fields' in schema_info:\n                    for field in schema_info['fields']:\n                        if 'name' in field and 'type' in field:\n                            column_types[field['name']] = field['type']\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\n# Check for statistics that might indicate encoding opportunities\ncolumn_stats = {}\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'add_' in col and '_stats' in col and isinstance(row[col], str):\n            try:\n                stats = json.loads(row[col].replace(\"'\", '\"'))\n                if 'stats' in stats:\n                    for col_name, col_stat in stats['stats'].items():\n                        if 'distinctCount' in col_stat and col_name in column_types:\n                            if col_name not in column_stats or 'distinctCount' not in column_stats[col_name]:\n                                column_stats[col_name] = {'distinctCount': col_stat['distinctCount']}\n                            elif col_stat['distinctCount'] > column_stats[col_name]['distinctCount']:\n                                column_stats[col_name]['distinctCount'] = col_stat['distinctCount']\n            except:\n                pass\n\n# Identify optimization opportunities\nissue_detected = False\nif column_types and column_stats:\n    dictionary_candidates = []\n    rle_candidates = []\n    delta_candidates = []\n    \n    for col_name, col_type in column_types.items():\n        # Check for dictionary encoding candidates (low cardinality columns)\n        if col_name in column_stats:\n            distinct_count = column_stats[col_name].get('distinctCount', 0)\n            \n            # String columns with low distinct counts are good for dictionary encoding\n            if isinstance(col_type, str) and ('string' in col_type.lower() or 'varchar' in col_type.lower()) and distinct_count > 0 and distinct_count < 1000:\n                dictionary_candidates.append((col_name, distinct_count))\n                issue_detected = True\n                \n            # Boolean or small int columns are good for RLE encoding\n            if isinstance(col_type, str) and ('boolean' in col_type.lower() or 'tinyint' in col_type.lower()) and distinct_count > 0 and distinct_count <= 10:\n                rle_candidates.append((col_name, distinct_count))\n                issue_detected = True\n                \n            # Integer columns are good for delta encoding\n            if isinstance(col_type, str) and any(x in col_type.lower() for x in ['int', 'long', 'timestamp', 'date']):\n                delta_candidates.append(col_name)\n                issue_detected = True\n    \n    # Report findings\n    if dictionary_candidates:\n        print(\"Columns suitable for dictionary encoding (low cardinality strings):\")\n        for col, count in sorted(dictionary_candidates, key=lambda x: x[1])[:5]:\n            print(f\"    '{col}' ({count} distinct values)\")\n            \n    if rle_candidates:\n        print(\"Columns suitable for run-length encoding (very low cardinality):\")\n        for col, count in rle_candidates[:5]:\n            print(f\"    '{col}' ({count} distinct values)\")\n            \n    if delta_candidates:\n        print(\"Columns suitable for delta encoding (numeric sequences):\")\n        for col in delta_candidates[:5]:\n            print(f\"    '{col}'\")\n            \n    if not dictionary_candidates and not rle_candidates and not delta_candidates:\n        print(\"No specific column encoding recommendations based on available statistics\")\nelse:\n    print(\"Insufficient schema or statistics data to analyze encoding efficiency\")\n    issue_detected = True\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Use appropriate Parquet encodings: dictionary encoding for low-cardinality columns, run-length encoding for binary/boolean fields, and delta encoding for numeric sequences.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL013",
      "category": "Schema",
      "description": "Check for data type optimization opportunities.",
      "pyspark_query": "# Analyze data type optimization opportunities\n# Extract schema\nschema_info = None\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\n# Extract stats to examine actual data characteristics\ncolumn_stats = {}\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'add_' in col and '_stats' in col and isinstance(row[col], str):\n            try:\n                stats = json.loads(row[col].replace(\"'\", '\"'))\n                if 'stats' in stats:\n                    for col_name, col_stat in stats['stats'].items():\n                        if col_name not in column_stats:\n                            column_stats[col_name] = {}\n                        for stat_name, stat_value in col_stat.items():\n                            column_stats[col_name][stat_name] = stat_value\n            except:\n                pass\n\noptimization_opportunities = []\nissue_detected = False\n\nif schema_info and 'fields' in schema_info:\n    # Check each column for optimization opportunities\n    for field in schema_info['fields']:\n        if 'name' in field and 'type' in field:\n            col_name = field['name']\n            data_type = str(field['type'])\n            \n            # Check if we have stats for this column\n            if col_name in column_stats:\n                stats = column_stats[col_name]\n                \n                # String columns that could be much smaller\n                if 'string' in data_type.lower() or 'varchar' in data_type.lower():\n                    if 'maxLen' in stats and 'minLen' in stats:\n                        max_len = stats['maxLen']\n                        if max_len < 10 and stats.get('distinctCount', 0) < 100:\n                            optimization_opportunities.append(f\"Column '{col_name}' uses string but could use an enum/categorical type (max length: {max_len}, distinct values: {stats.get('distinctCount', 'unknown')})\")\n                            issue_detected = True\n                        elif 'varchar' in data_type.lower() and '(max)' in data_type.lower() and max_len < 256:\n                            optimization_opportunities.append(f\"Column '{col_name}' uses varchar(max) but actual max length is only {max_len} characters\")\n                            issue_detected = True\n                            \n                # Numeric columns that could be smaller\n                if 'decimal' in data_type.lower():\n                    # Extract precision from type like decimal(p,s)\n                    precision_match = re.search(r'decimal\\(([0-9]+),([0-9]+)\\)', data_type.lower())\n                    if precision_match:\n                        precision = int(precision_match.group(1))\n                        scale = int(precision_match.group(2))\n                        \n                        if 'min' in stats and 'max' in stats:\n                            min_val = stats['min']\n                            max_val = stats['max']\n                            \n                            # Check if a smaller precision would work\n                            required_digits = max(len(str(int(abs(min_val)))), len(str(int(abs(max_val)))))\n                            if precision > required_digits + scale + 5:  # Allow some buffer\n                                optimization_opportunities.append(f\"Column '{col_name}' uses decimal({precision},{scale}) but values only need ~{required_digits+scale} digits\")\n                                issue_detected = True\n                                \n                # Integer columns that could be smaller (bigint → int → smallint)\n                if 'bigint' in data_type.lower() or 'int8' in data_type.lower():\n                    if 'min' in stats and 'max' in stats:\n                        min_val = stats['min']\n                        max_val = stats['max']\n                        \n                        if min_val >= -2147483648 and max_val <= 2147483647:\n                            optimization_opportunities.append(f\"Column '{col_name}' uses BIGINT but INT would be sufficient (range: {min_val} to {max_val})\")\n                            issue_detected = True\n                        elif min_val >= -32768 and max_val <= 32767:\n                            optimization_opportunities.append(f\"Column '{col_name}' uses BIGINT but SMALLINT would be sufficient (range: {min_val} to {max_val})\")\n                            issue_detected = True\n                        elif min_val >= 0 and max_val <= 65535:\n                            optimization_opportunities.append(f\"Column '{col_name}' uses BIGINT but UNSIGNED SMALLINT would be sufficient (range: {min_val} to {max_val})\")\n                            issue_detected = True\n\n    # Report findings\n    if optimization_opportunities:\n        print(f\"Found {len(optimization_opportunities)} data type optimization opportunities:\")\n        for i, opportunity in enumerate(optimization_opportunities[:5]):\n            print(f\"    {opportunity}\")\n        if len(optimization_opportunities) > 5:\n            print(f\"    ...and {len(optimization_opportunities) - 5} more opportunities\")\n    else:\n        print(\"No obvious data type optimization opportunities identified\")\nelse:\n    print(\"Could not find schema information in delta log\")\n    issue_detected = True\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Optimize data types to reduce storage and improve performance. Use the smallest suitable numeric types and appropriate string lengths.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL014",
      "category": "Delta Lake",
      "description": "Check for Delta checkpoint creation patterns.",
      "pyspark_query": "# Analyze Delta checkpointing patterns\n# Find checkpoint information in the delta logs\ncheckpoints = []\nmax_version = 0\n\n# Look for versions where checkpoints were created\nfor _, row in df_all.iterrows():\n    version = row.get('log_version')\n    max_version = max(max_version, version) if version is not None else max_version\n    \n    # Check if this version has a checkpoint\n    has_checkpoint = False\n    for col in row.index:\n        if col.startswith('commitInfo_') and isinstance(row[col], str) and 'checkpoint' in row[col].lower():\n            has_checkpoint = True\n            break\n    \n    if has_checkpoint:\n        checkpoints.append(version)\n\n# Check if number of changes between checkpoints is reasonable\nissue_detected = False\nif checkpoints:\n    checkpoint_intervals = []\n    for i in range(1, len(checkpoints)):\n        checkpoint_intervals.append(checkpoints[i] - checkpoints[i-1])\n    \n    avg_interval = sum(checkpoint_intervals) / len(checkpoint_intervals) if checkpoint_intervals else 0\n    last_checkpoint = checkpoints[-1]\n    versions_since_checkpoint = max_version - last_checkpoint\n    \n    print(f\"Found {len(checkpoints)} checkpoints in delta log\")\n    print(f\"Last checkpoint at version: {last_checkpoint}\")\n    print(f\"Current version: {max_version}\")\n    print(f\"Versions since last checkpoint: {versions_since_checkpoint}\")\n    \n    if checkpoint_intervals:\n        print(f\"Average checkpoint interval: {avg_interval:.1f} versions\")\n    \n    if versions_since_checkpoint > 100:\n        print(\"WARNING: Many versions since last checkpoint. This may impact table loading performance.\")\n        print(\"Consider running VACUUM to trigger checkpoint creation\")\n        issue_detected = True\nelse:\n    print(\"No checkpoints found in available delta log history\")\n    issue_detected = True\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Monitor checkpoint creation to ensure optimal table loading performance. Checkpoints reduce log replay time during table initialization.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL015",
      "category": "Maintenance",
      "description": "Check when VACUUM was last performed on the table.",
      "pyspark_query": "# Analyze vacuum operations history\n# Look for vacuum operations in delta log\nvacuum_timestamps = []\ncurrent_time = datetime.now().timestamp() * 1000  # Current time in ms\n\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'commitInfo' in col and isinstance(row[col], str) and 'operation' in row[col]:\n            try:\n                commit_info = json.loads(row[col].replace(\"'\", '\"'))\n                if commit_info.get('operation') == 'VACUUM':\n                    if 'timestamp' in commit_info:\n                        vacuum_timestamps.append(commit_info['timestamp'])\n            except Exception as e:\n                pass\n\n# Report vacuum findings\nissue_detected = False\nif vacuum_timestamps:\n    # Find most recent vacuum operation\n    most_recent = max(vacuum_timestamps)\n    days_since_vacuum = (current_time - most_recent) / (86400 * 1000)  # Convert ms to days\n    \n    vacuum_date = datetime.fromtimestamp(most_recent/1000).strftime('%Y-%m-%d')\n    print(f\"Last VACUUM operation: {vacuum_date} ({round(days_since_vacuum, 1)} days ago)\")\n    \n    if days_since_vacuum > 7:\n        print(\"No VACUUM operation in past 7 days\")\n        issue_detected = True\nelse:\n    print(\"No VACUUM operations found in delta log history\")\n    issue_detected = True\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Run VACUUM operations regularly (at least once a week) to remove stale files and maintain performance. Use VACUUM with 'DRY RUN' first to check what would be deleted.",
      "remediation_template": "VACUUM {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL016",
      "category": "Delta Lake",
      "description": "Check for auto-optimize feature enablement.",
      "pyspark_query": "# Check Auto-Optimize settings\n# Extract configuration from metadata\nauto_optimize = False\n\nfor _, row in df_all.iterrows():\n    # Check for Auto-Optimize setting\n    if 'metadata_configuration' in row:\n        try:\n            if isinstance(row['metadata_configuration'], str):\n                config = json.loads(row['metadata_configuration'].replace(\"'\", '\"'))\n                if 'autoOptimize' in config:\n                    auto_optimize = config['autoOptimize'].get('enabled', False)\n                break\n        except:\n            pass\n\n# Report findings\nissue_detected = not auto_optimize\nprint(f\"Auto-Optimize enabled: {auto_optimize}\")\nif not auto_optimize:\n    print(\"FINDING: Auto-Optimize is disabled\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Enable Auto-Optimize for frequently updated tables to automatically combine small files during write operations, improving future query performance.",
      "remediation_template": "ALTER TABLE {full_table_name} SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true', 'delta.autoOptimize.autoCompact' = 'true')",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL017",
      "category": "Delta Lake",
      "description": "Check data skipping settings for query performance.",
      "pyspark_query": "# Check data skipping settings\n# Extract configuration from metadata\ndata_skipping_enabled = False\ndata_skipping_num_cols = 0\n\nfor _, row in df_all.iterrows():\n    # Check for data skipping settings\n    if 'metadata_configuration' in row:\n        try:\n            if isinstance(row['metadata_configuration'], str):\n                config = json.loads(row['metadata_configuration'].replace(\"'\", '\"'))\n                if 'dataSkippingNumIndexedCols' in config:\n                    data_skipping_num_cols = int(config['dataSkippingNumIndexedCols'])\n                    data_skipping_enabled = data_skipping_num_cols > 0\n                break\n        except:\n            pass\n\n# Report findings\nissue_detected = not data_skipping_enabled or data_skipping_num_cols < 32\nprint(f\"Data skipping enabled: {data_skipping_enabled}\")\nif not data_skipping_enabled:\n    print(\"FINDING: Data skipping is not enabled\")\nelif data_skipping_num_cols < 32:\n    print(f\"FINDING: Data skipping only configured for {data_skipping_num_cols} columns (recommended: 32)\")\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Enable data skipping on large tables to improve query performance by allowing the query engine to skip reading irrelevant data files based on statistics.",
      "remediation_template": "ALTER TABLE {full_table_name} SET TBLPROPERTIES ('delta.dataSkippingNumIndexedCols' = '32')",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL018",
      "category": "Performance",
      "description": "Analyze Parquet row group sizes and settings.",
      "duckdb_query": "WITH row_group_stats AS (\n  SELECT \n    AVG(row_group_bytes) AS avg_bytes,\n    ROUND(AVG(row_group_bytes) / (1024*1024), 1) AS avg_mb\n  FROM df\n)\nSELECT \n  CONCAT('Row group size analysis: ', \n         CASE \n           WHEN avg_bytes < 16*1024*1024 THEN 'ISSUE: Row groups too small (' || avg_mb || ' MB avg). This reduces read efficiency.'\n           WHEN avg_bytes > 512*1024*1024 THEN 'ISSUE: Row groups too large (' || avg_mb || ' MB avg). This consumes excessive memory.'\n           ELSE 'Good configuration. Row groups average ' || avg_mb || ' MB (recommended: 128-256 MB).'\n         END, \n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN avg_bytes BETWEEN 16*1024*1024 AND 512*1024*1024 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM row_group_stats;",
      "recommendation": "Aim for row group sizes between 128MB and 256MB. Too small row groups reduce read efficiency, while too large ones consume excessive memory during processing.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL019",
      "category": "Compression",
      "description": "Analyze compression efficiency by column.",
      "duckdb_query": "WITH compression_stats AS (\n  SELECT \n    path_in_schema,\n    compression,\n    SUM(total_compressed_size) AS compressed_size,\n    SUM(total_uncompressed_size) AS uncompressed_size,\n    (1 - (SUM(total_compressed_size) / NULLIF(SUM(total_uncompressed_size), 0))) * 100 AS compression_ratio_pct\n  FROM df\n  WHERE total_uncompressed_size > 0\n  GROUP BY path_in_schema, compression\n),\navg_compression AS (\n  SELECT AVG(compression_ratio_pct) AS avg_ratio FROM compression_stats\n)\nSELECT\n  CONCAT('Compression efficiency: ', \n         CASE \n           WHEN (SELECT COUNT(*) FROM compression_stats) = 0 THEN 'No compression data available.'\n           WHEN (SELECT MIN(compression_ratio_pct) FROM compression_stats) < 25 THEN 'POOR compression found. Some columns have less than 25% compression efficiency. Best column achieves ' || ROUND((SELECT MAX(compression_ratio_pct) FROM compression_stats), 0) || '% reduction.'\n           WHEN (SELECT AVG(compression_ratio_pct) FROM compression_stats) BETWEEN 25 AND 60 THEN 'AVERAGE compression efficiency (' || ROUND((SELECT AVG(compression_ratio_pct) FROM compression_stats), 0) || '%). Consider optimizing encodings for better results.'\n           ELSE 'GOOD overall compression efficiency (' || ROUND((SELECT AVG(compression_ratio_pct) FROM compression_stats), 0) || '% space savings on average).'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN (SELECT COUNT(*) FROM compression_stats) = 0 OR (SELECT AVG(compression_ratio_pct) FROM compression_stats) >= 25 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output;",
      "recommendation": "Check columns with poor compression ratios. For numeric columns with low compression efficiency, consider using appropriate encoding types like DELTA or RLE. For string columns, dictionary encoding may help.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL020",
      "category": "Performance",
      "description": "Analyze encoding types used by columns.",
      "duckdb_query": "WITH missing_encodings AS (\n  SELECT \n    COUNT(CASE WHEN type LIKE '%ARRAY%' AND encodings NOT LIKE '%DICTIONARY%' THEN 1 END) AS string_without_dict,\n    COUNT(CASE WHEN type LIKE 'INT%' AND encodings NOT LIKE '%DELTA%' THEN 1 END) AS int_without_delta,\n    COUNT(CASE WHEN type LIKE 'BOOLEAN' AND encodings NOT LIKE '%RLE%' THEN 1 END) AS bool_without_rle,\n    COUNT(*) AS total_columns\n  FROM (\n    SELECT path_in_schema, type, encodings\n    FROM df\n    GROUP BY path_in_schema, type, encodings\n  )\n)\nSELECT\n  CONCAT('Encoding optimization: ', \n         CASE\n           WHEN string_without_dict + int_without_delta + bool_without_rle = 0 THEN 'All columns use appropriate encodings.'\n           ELSE 'Found ' || \n                (CASE WHEN string_without_dict > 0 THEN string_without_dict || ' string columns missing dictionary encoding' ELSE '' END) ||\n                (CASE WHEN int_without_delta > 0 AND string_without_dict > 0 THEN ', ' ELSE '' END) ||\n                (CASE WHEN int_without_delta > 0 THEN int_without_delta || ' numeric columns missing delta encoding' ELSE '' END) ||\n                (CASE WHEN bool_without_rle > 0 AND (string_without_dict > 0 OR int_without_delta > 0) THEN ', ' ELSE '' END) ||\n                (CASE WHEN bool_without_rle > 0 THEN bool_without_rle || ' boolean columns missing RLE encoding' ELSE '' END) ||\n                '. Proper encodings would improve compression and performance.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN string_without_dict + int_without_delta + bool_without_rle = 0 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM missing_encodings;",
      "recommendation": "For string columns, DICTIONARY encoding works well for low-cardinality values. For ID columns, DELTA encoding is efficient. For boolean/flags, RLE (run-length encoding) provides best compression.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL021",
      "category": "Performance",
      "description": "Check for missing statistics that affect query pruning.",
      "duckdb_query": "WITH stats_summary AS (\n  SELECT\n    COUNT(DISTINCT CASE WHEN stats_min IS NULL OR stats_max IS NULL OR stats_null_count IS NULL THEN path_in_schema END) AS columns_with_missing_stats,\n    COUNT(DISTINCT CASE WHEN stats_min IS NULL AND stats_max IS NULL THEN path_in_schema END) AS columns_missing_minmax,\n    COUNT(DISTINCT CASE WHEN stats_null_count IS NULL THEN path_in_schema END) AS columns_missing_nullcount,\n    COUNT(DISTINCT path_in_schema) AS total_columns_checked\n  FROM df\n)\nSELECT\n  CONCAT('Statistics coverage: ', \n         CASE\n           WHEN columns_with_missing_stats = 0 THEN 'Complete statistics available for all columns. Optimal query pruning possible.'\n           WHEN columns_missing_minmax > 0 THEN 'CRITICAL: ' || columns_missing_minmax || ' columns missing min/max statistics. This prevents range filtering and efficient data skipping.'\n           WHEN columns_missing_nullcount > 0 THEN 'WARNING: ' || columns_missing_nullcount || ' columns missing null counts. This affects null handling in queries.'\n           ELSE 'Some statistics missing for ' || columns_with_missing_stats || ' columns. Query pruning partially affected.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN columns_with_missing_stats = 0 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM stats_summary;",
      "recommendation": "Missing statistics prevent the query engine from skipping irrelevant data. Run ANALYZE or similar commands to generate statistics for columns frequently used in filters or joins.",
      "remediation_template": "ANALYZE TABLE {full_table_name} COMPUTE STATISTICS",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL022",
      "category": "Performance",
      "description": "Check Bloom filter usage across columns.",
      "duckdb_query": "WITH key_columns_all AS (\n  SELECT\n    path_in_schema\n  FROM df\n  WHERE REGEXP_MATCHES(LOWER(path_in_schema), '(^|_)(id|key|code|pk)($|_)')\n  GROUP BY path_in_schema\n),\nkey_summary AS (\n  SELECT\n    COUNT(*) AS key_columns_count,\n    (SELECT COUNT(DISTINCT path_in_schema) FROM df) AS total_columns\n  FROM key_columns_all\n),\nkey_examples AS (\n  SELECT path_in_schema\n  FROM key_columns_all\n  ORDER BY path_in_schema\n  LIMIT 3\n)\nSELECT\n  CONCAT('Bloom filter candidates: ', \n         CASE\n           WHEN key_columns_count = 0 THEN 'No key columns detected. Bloom filters may not be necessary.'\n           ELSE 'Found ' || key_columns_count || ' key columns (e.g., ' || \n                (SELECT STRING_AGG(path_in_schema, ', ') FROM key_examples) || \n                (CASE WHEN key_columns_count > 3 THEN ', ...' ELSE '' END) || \n                ') that would benefit from Bloom filters for faster joins and equality filters.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN key_columns_count = 0 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM key_summary;",
      "recommendation": "Bloom filters accelerate point lookups and equi-joins. For high-cardinality columns used in equality predicates (WHERE col = value), enable Bloom filters to dramatically improve query performance.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL023",
      "category": "Schema",
      "description": "Analyze column data types and their storage implications.",
      "duckdb_query": "WITH type_stats AS (\n  SELECT\n    type,\n    COUNT(DISTINCT path_in_schema) AS column_count,\n    ROUND(SUM(total_compressed_size) / (1024*1024), 2) AS total_size_mb\n  FROM df\n  GROUP BY type\n)\nSELECT\n  CONCAT('Data type efficiency: ', \n         CASE \n           WHEN COUNT(*) = 0 THEN 'No type information available.'\n           WHEN SUM(total_size_mb) < 1 THEN 'Small table (' || ROUND(SUM(total_size_mb), 1) || ' MB), type optimization not critical.'\n           WHEN SUM(CASE WHEN type LIKE '%BYTE_ARRAY%' THEN total_size_mb ELSE 0 END) > 0.7 * SUM(total_size_mb) THEN\n             'String columns dominate storage (' || \n             ROUND(SUM(CASE WHEN type LIKE '%BYTE_ARRAY%' THEN total_size_mb ELSE 0 END) * 100.0 / SUM(total_size_mb), 0) || \n             '% of space). Consider string optimization or categorical types.'\n           WHEN SUM(CASE WHEN type LIKE '%INT64%' THEN column_count ELSE 0 END) > 3 THEN\n             'Found ' || SUM(CASE WHEN type LIKE '%INT64%' THEN column_count ELSE 0 END) || \n             ' INT64 columns. Consider smaller integer types (INT32/16/8) if value ranges allow.'\n           ELSE 'Data type distribution appears reasonable across ' || SUM(column_count) || ' columns.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN COUNT(*) = 0 OR SUM(total_size_mb) < 1 THEN 'Optimized - OPT_2001'\n           WHEN SUM(CASE WHEN type LIKE '%BYTE_ARRAY%' THEN total_size_mb ELSE 0 END) > 0.7 * SUM(total_size_mb) THEN 'Anomaly - ERR_1001'\n           WHEN SUM(CASE WHEN type LIKE '%INT64%' THEN column_count ELSE 0 END) > 3 THEN 'Anomaly - ERR_1001'\n           ELSE 'Optimized - OPT_2001'\n         END) AS output\nFROM type_stats;",
      "recommendation": "Review large average column sizes. Consider more efficient types: use INT instead of BIGINT where possible, SMALL/TINYINT for limited ranges, DATE instead of TIMESTAMP when time not needed, and enumerations for recurring string values.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL024",
      "category": "Performance",
      "description": "Analyze file size distribution for small file problems.",
      "duckdb_query": "WITH file_sizes AS (\n  SELECT\n    COUNT(*) AS total_files,\n    COUNT(CASE WHEN size_mb < 25 THEN 1 END) AS small_files,\n    COUNT(CASE WHEN size_mb BETWEEN 25 AND 150 THEN 1 END) AS medium_files,\n    COUNT(CASE WHEN size_mb > 150 THEN 1 END) AS large_files,\n    ROUND(AVG(size_mb), 1) AS avg_file_size_mb\n  FROM (\n    SELECT\n      file_name,\n      SUM(row_group_bytes) / (1024*1024) AS size_mb\n    FROM df\n    GROUP BY file_name\n  )\n)\nSELECT\n  CONCAT('File size distribution: ', \n         CASE\n           WHEN total_files = 0 THEN 'No file size information available.'\n           WHEN small_files > 0.5 * total_files THEN 'CRITICAL: ' || small_files || ' of ' || total_files || \n                                                   ' files (' || ROUND(small_files * 100.0 / total_files, 0) || \n                                                   '%) are too small (<25MB). Run OPTIMIZE to combine small files.'\n           WHEN small_files > 0.2 * total_files THEN 'CONCERNING: ' || small_files || ' of ' || total_files || \n                                                    ' files (' || ROUND(small_files * 100.0 / total_files, 0) || \n                                                    '%) are small (<25MB). Consider file compaction.'\n           WHEN large_files = total_files AND total_files > 1 THEN 'NOTE: All files are large (>150MB). Check if too large for efficient parallel processing.'\n           ELSE 'GOOD: File sizes mostly in optimal range. Average file size: ' || avg_file_size_mb || ' MB.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN total_files = 0 OR small_files <= 0.2 * total_files THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM file_sizes;",
      "recommendation": "Address small file problems by running OPTIMIZE or combining data during write operations. Aim for file sizes between 25-150MB for optimal processing. Too many small files cause excessive overhead.",
      "remediation_template": "OPTIMIZE {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL025",
      "category": "Data Quality",
      "description": "Check for high cardinality columns that might impact performance.",
      "duckdb_query": "WITH distinct_counts AS (\n  -- Get distinct value estimates from available statistics\n  SELECT\n    path_in_schema,\n    MAX(COALESCE(stats_distinct_count, 0)) AS distinct_count,\n    MAX(COALESCE(row_group_num_rows, 0)) AS row_count\n  FROM df\n  GROUP BY path_in_schema\n),\ncolumn_analysis AS (\n  SELECT\n    path_in_schema,\n    distinct_count,\n    row_count,\n    CASE \n      WHEN row_count = 0 THEN 0\n      ELSE (distinct_count * 100.0 / row_count)\n    END AS cardinality_pct,\n    CASE\n      WHEN row_count = 0 THEN 'Unknown'\n      WHEN (distinct_count * 100.0 / NULLIF(row_count, 0)) > 90 THEN 'Very High'\n      WHEN (distinct_count * 100.0 / NULLIF(row_count, 0)) BETWEEN 50 AND 90 THEN 'High'\n      WHEN (distinct_count * 100.0 / NULLIF(row_count, 0)) BETWEEN 10 AND 50 THEN 'Medium'\n      ELSE 'Low'\n    END AS cardinality_level\n  FROM distinct_counts\n  WHERE row_count > 0 AND distinct_count > 0\n),\nhigh_cardinality_count AS (\n  SELECT COUNT(*) AS very_high_count FROM column_analysis WHERE cardinality_level = 'Very High'\n),\ntop_columns AS (\n  SELECT STRING_AGG(path_in_schema || ' (' || ROUND(cardinality_pct, 1) || '%)', ', ') AS top_cols\n  FROM (SELECT path_in_schema, cardinality_pct FROM column_analysis ORDER BY cardinality_pct DESC LIMIT 3)\n)\nSELECT\n  CONCAT(\n    CASE\n      WHEN (SELECT COUNT(*) FROM column_analysis) = 0 THEN\n        'No cardinality statistics available in file metadata. Consider running ANALYZE to generate column statistics.'\n      ELSE\n        'Column cardinality analysis: ' ||\n        (SELECT very_high_count FROM high_cardinality_count) || ' columns with very high cardinality (>90%), ' ||\n        (SELECT COUNT(*) FROM column_analysis WHERE cardinality_level = 'High') || ' columns with high cardinality (50-90%), ' ||\n        (SELECT COUNT(*) FROM column_analysis WHERE cardinality_level = 'Medium') || ' columns with medium cardinality (10-50%), ' ||\n        (SELECT COUNT(*) FROM column_analysis WHERE cardinality_level = 'Low') || ' columns with low cardinality (<10%). ' ||\n        'Top high cardinality columns: ' ||\n        COALESCE((SELECT top_cols FROM top_columns), 'none')\n    END,\n    '\n\n',\n    'Indicator: ',\n    CASE\n      WHEN (SELECT COUNT(*) FROM column_analysis) = 0 OR (SELECT very_high_count FROM high_cardinality_count) = 0 THEN 'Optimized - OPT_2001'\n      ELSE 'Anomaly - ERR_1001'\n    END) AS output;",
      "recommendation": "High cardinality string columns with many unique values may benefit from dictionary encoding with larger dictionary sizes. For columns with cardinality close to row count (almost unique), avoid using them in GROUP BY unless necessary. Low cardinality columns (<10% unique values) are good candidates for filtering and compression.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL026",
      "category": "Performance",
      "description": "Analyze column chunk distribution across row groups for query optimization.",
      "duckdb_query": "WITH chunk_analysis AS (\n  SELECT\n    path_in_schema,\n    COUNT(DISTINCT row_group_id) AS groups_with_column,\n    (SELECT COUNT(DISTINCT row_group_id) FROM df) AS total_row_groups,\n    ROUND(COUNT(DISTINCT row_group_id) * 100.0 / NULLIF((SELECT COUNT(DISTINCT row_group_id) FROM df), 0), 1) AS coverage_pct\n  FROM df\n  GROUP BY path_in_schema\n),\ncolumn_distribution AS (\n  SELECT\n    MIN(coverage_pct) AS min_coverage,\n    MAX(coverage_pct) AS max_coverage,\n    AVG(coverage_pct) AS avg_coverage,\n    COUNT(CASE WHEN coverage_pct < 100 THEN 1 END) AS incomplete_columns,\n    COUNT(CASE WHEN coverage_pct = 100 THEN 1 END) AS complete_columns,\n    COUNT(*) AS total_columns\n  FROM chunk_analysis\n),\nproblematic_cols AS (\n  SELECT STRING_AGG(path_in_schema || ' (' || ROUND(coverage_pct, 1) || '%)', ', ') AS problem_cols\n  FROM (\n    SELECT path_in_schema, coverage_pct\n    FROM chunk_analysis\n    WHERE coverage_pct < 100\n    ORDER BY coverage_pct ASC\n    LIMIT 3\n  )\n)\nSELECT\n  CONCAT('Column chunk distribution: ', \n         CASE\n           WHEN (SELECT total_columns FROM column_distribution) = 0 THEN 'No column distribution data available.'\n           WHEN (SELECT incomplete_columns FROM column_distribution) = 0 THEN 'OPTIMAL: All ' || (SELECT total_columns FROM column_distribution) || ' columns present in all row groups. Even column distribution for efficient querying.'\n           WHEN (SELECT incomplete_columns FROM column_distribution) > 0 AND (SELECT min_coverage FROM column_distribution) < 50 THEN\n             'INCONSISTENT distribution detected. ' || (SELECT incomplete_columns FROM column_distribution) || ' of ' || (SELECT total_columns FROM column_distribution) || \n             ' columns not present in all row groups. Worst cases: ' ||\n             (SELECT problem_cols FROM problematic_cols) ||\n             '. This indicates potential split file writing or improper column pruning.'\n           ELSE\n             'MINOR VARIATION: ' || (SELECT incomplete_columns FROM column_distribution) || ' columns have some variation in row group presence (' ||\n             ROUND((SELECT min_coverage FROM column_distribution), 1) || '% to ' || ROUND((SELECT max_coverage FROM column_distribution), 1) || '% coverage), but this is usually not problematic.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN (SELECT total_columns FROM column_distribution) = 0 OR (SELECT incomplete_columns FROM column_distribution) = 0 THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output;",
      "recommendation": "Ensure columns are consistently distributed across row groups. Inconsistent column presence in row groups often indicates split writing operations or improper column pruning. This can cause inefficient query execution, as the engine must navigate uneven distributions.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL027",
      "category": "Performance",
      "description": "Analyze row group count versus file size for parallelism optimization.",
      "duckdb_query": "WITH file_metrics AS (\n  SELECT\n    file_name,\n    COUNT(DISTINCT row_group_id) AS row_groups,\n    SUM(row_group_num_rows) AS row_count,\n    SUM(row_group_bytes) / (1024*1024) AS file_size_mb,\n    SUM(row_group_bytes) / COUNT(DISTINCT row_group_id) / (1024*1024) AS mb_per_row_group\n  FROM df\n  GROUP BY file_name\n),\nparallelism_metrics AS (\n  SELECT\n    COUNT(*) AS file_count,\n    ROUND(AVG(row_groups), 1) AS avg_row_groups_per_file,\n    SUM(row_groups) AS total_row_groups,\n    ROUND(AVG(file_size_mb), 1) AS avg_file_size_mb,\n    ROUND(AVG(mb_per_row_group), 1) AS avg_rowgroup_size_mb,\n    MIN(row_groups) AS min_row_groups,\n    MAX(row_groups) AS max_row_groups,\n    ROUND(SUM(file_size_mb), 1) AS total_size_mb,\n    SUM(row_count) AS total_rows\n  FROM file_metrics\n)\nSELECT\n  CONCAT('Parallelism optimization: ', \n         CASE \n           WHEN file_count = 0 THEN 'No file metrics available for analysis.'\n           WHEN file_count = 1 AND total_row_groups < 4 AND total_size_mb > 1000 THEN\n             'PARALLELISM BOTTLENECK: Single large file (' || total_size_mb || ' MB) with only ' || \n             total_row_groups || ' row groups. Consider increasing row groups to at least 8 for better parallelism.'\n           WHEN avg_row_groups_per_file < 2 AND file_count < 8 AND total_size_mb > 200 THEN\n             'LIMITED PARALLELISM: Only ' || total_row_groups || ' row groups across ' || file_count || \n             ' files (' || total_size_mb || ' MB total). Increase row group count for better query performance.'\n           WHEN avg_rowgroup_size_mb < 16 AND total_row_groups > 32 AND file_count > 1 THEN\n             'EXCESSIVE PARALLELISM: Many small row groups (' || total_row_groups || ' groups, ' || \n             avg_rowgroup_size_mb || ' MB each). This may cause coordination overhead. Consider merging row groups.'\n           WHEN total_row_groups >= 8 AND avg_rowgroup_size_mb BETWEEN 32 AND 256 THEN\n             'OPTIMAL PARALLELISM: Good distribution with ' || total_row_groups || ' row groups averaging ' || \n             avg_rowgroup_size_mb || ' MB each, allowing efficient parallel processing.'\n           ELSE 'ACCEPTABLE PARALLELISM: ' || total_row_groups || ' row groups across ' || file_count || ' files with average row group size of ' || avg_rowgroup_size_mb || ' MB.'\n         END,\n         '\n\n',\n         'Indicator: ',\n         CASE\n           WHEN file_count = 0 OR (total_row_groups >= 8 AND avg_rowgroup_size_mb BETWEEN 32 AND 256) THEN 'Optimized - OPT_2001'\n           ELSE 'Anomaly - ERR_1001'\n         END) AS output\nFROM parallelism_metrics;",
      "recommendation": "Optimize row group count for query parallelism. For tables larger than 1GB, aim for at least 8 row groups total (across all files) to enable efficient parallel processing. Very large tables benefit from 16-32 row groups. However, excessive row groups (>100) can cause coordination overhead. Balance parallelism against row group size (aim for 64-256 MB per row group).",
      "remediation_template": "OPTIMIZE {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL028",
      "category": "Schema",
      "description": "Check for presence of standard audit columns (created_at, updated_at, created_by, updated_by)",
      "duckdb_query": "WITH audit_columns AS (\n  SELECT column_name FROM (\n    VALUES ('created_at'), ('updated_at'), ('created_by'), ('updated_by')\n  ) AS cols(column_name)\n),\ncolumn_names AS (\n  SELECT DISTINCT LOWER(path_in_schema) AS column_name\n  FROM df\n),\nmissing_columns AS (\n  SELECT audit_columns.column_name\n  FROM audit_columns\n  LEFT JOIN column_names ON audit_columns.column_name = column_names.column_name\n  WHERE column_names.column_name IS NULL\n),\nmissing_count AS (\n  SELECT COUNT(*) AS count, \n         STRING_AGG(column_name, ', ') AS missing_list\n  FROM missing_columns\n)\nSELECT\n  CONCAT(\n    'Audit columns analysis: ',\n    CASE \n      WHEN (SELECT count FROM missing_count) = 0 THEN\n        'All standard audit columns are present. Good practice for data lineage and governance.'\n      WHEN (SELECT count FROM missing_count) = 4 THEN\n        'No audit columns found. Consider adding standard audit columns to track data lineage.'\n      ELSE\n        'Missing ' || (SELECT count FROM missing_count) || ' audit columns: ' ||\n        (SELECT missing_list FROM missing_count) || '. These columns help with data lineage and governance.'\n    END,\n    '\n\n',\n    'Indicator: ',\n    CASE\n      WHEN (SELECT count FROM missing_count) = 0 THEN 'Optimized - OPT_2001'\n      ELSE 'Anomaly - ERR_1001'\n    END\n  ) AS output\nFROM missing_count;",
      "recommendation": "Add the missing audit columns `created_at`, `updated_at`, `created_by`, `updated_by` to support data lineage and troubleshooting.",
      "remediation_template": "ALTER TABLE {full_table_name} ADD COLUMNS (created_at TIMESTAMP, updated_at TIMESTAMP, created_by STRING, updated_by STRING)",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL029",
      "category": "Data Quality",
      "description": "Check for high null ratio in Parquet columns",
      "duckdb_query": "WITH column_nulls AS (\n  SELECT\n    path_in_schema,\n    SUM(stats_null_count) AS total_nulls,\n    SUM(row_group_num_rows) AS total_rows,\n    CASE \n      WHEN SUM(row_group_num_rows) > 0 THEN\n        ROUND(SUM(stats_null_count) * 100.0 / SUM(row_group_num_rows), 1)\n      ELSE 0\n    END AS null_ratio_pct\n  FROM df\n  GROUP BY path_in_schema\n  HAVING SUM(row_group_num_rows) > 0\n),\nhigh_null_columns AS (\n  SELECT\n    path_in_schema,\n    null_ratio_pct\n  FROM column_nulls\n  WHERE null_ratio_pct > 10\n  ORDER BY null_ratio_pct DESC\n),\nhigh_null_examples AS (\n  SELECT STRING_AGG(path_in_schema || ' (' || null_ratio_pct || '%)', ', ') AS problem_cols\n  FROM (\n    SELECT path_in_schema, null_ratio_pct FROM high_null_columns LIMIT 5\n  )\n)\nSELECT\n  CONCAT(\n    'Null value analysis: ',\n    CASE\n      WHEN (SELECT COUNT(*) FROM high_null_columns) = 0 THEN\n        'All columns have acceptable null ratios (≤10%).'\n      ELSE\n        'Found ' || (SELECT COUNT(*) FROM high_null_columns) || ' columns with high null ratios (>10%): ' ||\n        COALESCE((SELECT problem_cols FROM high_null_examples), '') ||\n        (CASE WHEN (SELECT COUNT(*) FROM high_null_columns) > 5 THEN ', and ' || ((SELECT COUNT(*) FROM high_null_columns) - 5) || ' more.' ELSE '' END) ||\n        ' High null ratios may indicate data quality issues or inefficient storage.'\n    END,\n    '\n\n',\n    'Indicator: ',\n    CASE\n      WHEN (SELECT COUNT(*) FROM high_null_columns) = 0 THEN 'Optimized - OPT_2001'\n      ELSE 'Anomaly - ERR_1001'\n    END\n  ) AS output;",
      "recommendation": "Address high null ratios by improving data collection, filling defaults, or using null-aware encodings.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "column"
    },
    {
      "id": "RL030",
      "category": "Delta Lake",
      "description": "Check Delta log version count for excessive history",
      "pyspark_query": "# Check delta log version count\nversion_count = len(df_all)\n\n# Define threshold for excessive versions\nexcessive_threshold = 500\n\n# Check if threshold is exceeded\nif version_count > excessive_threshold:\n    print(f\"FINDING: High number of Delta table versions: {version_count}\")\n    print(f\"Large version history can impact performance when loading the table\")\n    print(f\"Consider vacuuming old versions and implementing retention policies\")\n    issue_detected = True\nelse:\n    print(f\"Delta table version count is acceptable: {version_count}\")\n    if version_count > 100:\n        print(f\"Consider monitoring history growth over time\")\n    issue_detected = False\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Monitor and clean up history if versions exceed 500 to prevent performance degradation.",
      "remediation_template": "VACUUM {full_table_name} RETAIN 168 HOURS",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL031",
      "category": "Delta Lake",
      "description": "Check ratio of DELETE operations to total operations",
      "pyspark_query": "# Analyze DELETE operations in the delta log\ndelete_operations = 0\ntotal_operations = 0\n\n# Count operations in the delta log\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'commitInfo' in col and isinstance(row[col], str) and 'operation' in row[col]:\n            total_operations += 1\n            try:\n                commit_info = json.loads(row[col].replace(\"'\", '\"'))\n                if commit_info.get('operation') == 'DELETE':\n                    delete_operations += 1\n            except Exception:\n                pass\n\n# Calculate delete ratio\ndelete_ratio = delete_operations / total_operations if total_operations > 0 else 0\ndelete_ratio_pct = delete_ratio * 100\n\n# Define threshold for high delete ratio\nhigh_delete_threshold = 0.1  # 10%\n\n# Report findings\nprint(f\"Total operations in delta log: {total_operations}\")\nprint(f\"DELETE operations: {delete_operations} ({delete_ratio_pct:.1f}%)\")\n\n# Check if threshold is exceeded\nif delete_ratio > high_delete_threshold:\n    print(f\"FINDING: High delete ratio detected: {delete_ratio_pct:.1f}% of operations are DELETEs\")\n    print(f\"Consider enabling deletion vectors or periodic compaction for better performance\")\n    issue_detected = True\nelse:\n    print(f\"Delete ratio is acceptable: {delete_ratio_pct:.1f}% (threshold: {high_delete_threshold*100}%)\")\n    issue_detected = False\n\n# Add an empty line before the indicator\nprint(\"\")\n\n# Indicator block\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "If DELETE operations are frequent, consider enabling deletion vectors or periodic compaction to optimize performance.",
      "remediation_template": "ALTER TABLE {full_table_name} SET TBLPROPERTIES ('delta.enableDeletionVectors' = 'true')",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL032",
      "category": "Parquet File Structure",
      "description": "Check continuity of row-group file offsets within Parquet files",
      "duckdb_query": "WITH file_offsets AS (\n  SELECT\n    file_name, \n    row_group_id, \n    file_offset, \n    row_group_bytes,\n    LEAD(file_offset) OVER (PARTITION BY file_name ORDER BY file_offset) AS next_offset,\n    file_offset + row_group_bytes AS expected_next_offset\n  FROM df\n),\noffset_issues AS (\n  SELECT\n    file_name,\n    row_group_id,\n    next_offset,\n    expected_next_offset,\n    next_offset - expected_next_offset AS gap_bytes\n  FROM file_offsets\n  WHERE next_offset IS NOT NULL AND next_offset <> expected_next_offset\n),\ngap_summary AS (\n  SELECT\n    COUNT(*) AS issue_count,\n    COUNT(DISTINCT file_name) AS files_with_issues,\n    MAX(ABS(gap_bytes)) AS max_gap_bytes\n  FROM offset_issues\n)\nSELECT\n  CONCAT(\n    'Row-group offset continuity: ',\n    CASE\n      WHEN (SELECT issue_count FROM gap_summary) = 0 THEN\n        'All row-group offsets are continuous within files. Optimal file structure detected.'\n      ELSE\n        'FINDING: Detected ' || (SELECT issue_count FROM gap_summary) || ' discontinuities in row-group offsets across ' ||\n        (SELECT files_with_issues FROM gap_summary) || ' files. Maximum gap/overlap: ' ||\n        ROUND((SELECT max_gap_bytes FROM gap_summary) / 1024.0, 1) || ' KB. ' ||\n        'This can affect read efficiency and may indicate file corruption or improper writes.'\n    END,\n    '\n\n',\n    'Indicator: ',\n    CASE\n      WHEN (SELECT issue_count FROM gap_summary) = 0 THEN 'Optimized - OPT_2001'\n      ELSE 'Anomaly - ERR_1001'\n    END\n  ) AS output;",
      "recommendation": "Investigate and compact files if there are unexpected gaps or overlaps between row-groups, which can affect read efficiency.",
      "remediation_template": "OPTIMIZE {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL033",
      "category": "Performance",
      "description": "Check if V-Order optimization is enabled on the table.",
      "pyspark_query": "# Check V-Order optimization setting\nvorder_enabled = False\n\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_configuration' in col and isinstance(row[col], str):\n            try:\n                config = json.loads(row[col].replace(\"'\", '\"'))\n                if 'delta.vOrder.enabled' in config:\n                    vorder_enabled = str(config['delta.vOrder.enabled']).lower() == 'true'\n                break\n            except:\n                pass\n        if 'metadata' in col and isinstance(row[col], str) and 'vOrder' in row[col]:\n            if 'true' in row[col].lower():\n                vorder_enabled = True\n            break\n\nif vorder_enabled:\n    print(\"V-Order optimization is enabled. Data is sorted for optimal read performance in Fabric.\")\nelse:\n    print(\"V-Order optimization is NOT enabled.\")\n    print(\"V-Order significantly improves read performance in Microsoft Fabric by pre-sorting data.\")\n    print(\"Enable it with: spark.conf.set('spark.sql.parquet.vorder.enabled', 'true')\")\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if vorder_enabled else \"Anomaly - ERR_1001\"))",
      "recommendation": "Enable V-Order optimization for tables in Microsoft Fabric. V-Order pre-sorts Parquet data for significantly faster reads. Set spark.sql.parquet.vorder.enabled=true in your Spark session.",
      "remediation_template": "OPTIMIZE {full_table_name} VORDER",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL034",
      "category": "Performance",
      "description": "Check if Liquid Clustering is configured on the table (modern replacement for partitioning + Z-Order).",
      "pyspark_query": "# Check for Liquid Clustering configuration\nclustering_columns = None\n\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if isinstance(row[col], str) and 'clusteringColumns' in str(col):\n            try:\n                clustering_columns = json.loads(row[col].replace(\"'\", '\"')) if isinstance(row[col], str) else row[col]\n                break\n            except:\n                pass\n        if 'metadata_configuration' in col and isinstance(row[col], str) and 'clusteringColumns' in row[col]:\n            try:\n                config = json.loads(row[col].replace(\"'\", '\"'))\n                clustering_columns = config.get('clusteringColumns', None)\n                break\n            except:\n                pass\n    if clustering_columns:\n        break\n\nif clustering_columns and len(clustering_columns) > 0:\n    print(f\"Liquid Clustering is configured with columns: {', '.join(clustering_columns)}\")\n    print(\"This is the recommended modern approach for data layout optimization in Fabric.\")\nelse:\n    print(\"Liquid Clustering is not configured on this table.\")\n    print(\"Liquid Clustering is the recommended alternative to partitioning + Z-Order in Fabric.\")\n    print(\"It automatically manages data layout for optimal query performance.\")\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Optimized - OPT_2001\" if (clustering_columns and len(clustering_columns) > 0) else \"Anomaly - ERR_1001\"))",
      "recommendation": "Consider using Liquid Clustering instead of traditional partitioning + Z-Order. It automatically manages data layout and adapts to query patterns. Enable with: ALTER TABLE <table> CLUSTER BY (col1, col2).",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL035",
      "category": "Maintenance",
      "description": "Detect orphaned data files that are no longer referenced by the Delta log.",
      "pyspark_query": "# Detect orphaned files not tracked by Delta log\nimport os\n\n# Collect all file paths referenced in the delta log (add actions)\nreferenced_files = set()\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if col.startswith('add_') and '_path' in col and isinstance(row[col], str):\n            referenced_files.add(row[col])\n\n# Count referenced files\nnum_referenced = len(referenced_files)\n\n# Report findings\nif num_referenced == 0:\n    print(\"No file references found in Delta log. Cannot determine orphaned files.\")\n    issue_detected = True\nelse:\n    print(f\"Delta log references {num_referenced} data files.\")\n    print(\"Run VACUUM to remove any unreferenced orphaned files and reclaim storage.\")\n    issue_detected = False\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Run VACUUM regularly to remove orphaned files that are no longer referenced by the Delta log. Orphaned files waste storage and can slow metadata operations.",
      "remediation_template": "VACUUM {full_table_name}",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL036",
      "category": "Performance",
      "description": "Analyze write amplification by checking the ratio of MERGE/UPDATE/DELETE operations to WRITE/APPEND operations.",
      "pyspark_query": "# Analyze write amplification from commit log\nwrite_ops = 0\nmerge_update_ops = 0\ntotal_ops = 0\n\nfor _, row in df_all.iterrows():\n    op = row.get('summary_operation')\n    if op:\n        total_ops += 1\n        if op in ('WRITE', 'APPEND'):\n            write_ops += 1\n        elif op in ('MERGE', 'UPDATE', 'DELETE'):\n            merge_update_ops += 1\n\nif total_ops > 0:\n    merge_ratio = merge_update_ops / total_ops * 100\n    print(f\"Total operations: {total_ops}\")\n    print(f\"WRITE/APPEND operations: {write_ops} ({write_ops/total_ops*100:.1f}%)\")\n    print(f\"MERGE/UPDATE/DELETE operations: {merge_update_ops} ({merge_ratio:.1f}%)\")\n    \n    if merge_ratio > 50:\n        print(\"WARNING: High write amplification detected (>50% mutation operations).\")\n        print(\"Frequent MERGE/UPDATE/DELETE operations cause file rewrites and increase storage churn.\")\n        print(\"Consider batch updates, append-only patterns, or more frequent OPTIMIZE.\")\n        issue_detected = True\n    else:\n        print(\"Write amplification is within acceptable range.\")\n        issue_detected = False\nelse:\n    print(\"No operations found in Delta log history.\")\n    issue_detected = False\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "High write amplification (many MERGE/UPDATE/DELETE operations) causes excessive file rewrites. Consider append-only patterns, batch updates, or more frequent OPTIMIZE runs.",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL037",
      "category": "Schema",
      "description": "Detect schema drift by comparing schema changes across Delta log versions.",
      "pyspark_query": "# Detect schema drift across Delta log versions\nschemas = []\n\nfor _, row in df_all.iterrows():\n    version = row.get('log_version', 0)\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema = json.loads(row[col].replace(\"'\", '\"'))\n                if 'fields' in schema:\n                    field_names = sorted([f['name'] for f in schema['fields'] if 'name' in f])\n                    schemas.append({'version': version, 'fields': field_names, 'count': len(field_names)})\n            except:\n                pass\n\nif len(schemas) > 1:\n    first = schemas[0]\n    last = schemas[-1]\n    added = set(last['fields']) - set(first['fields'])\n    removed = set(first['fields']) - set(last['fields'])\n    \n    if added or removed:\n        print(f\"Schema drift detected across {len(schemas)} versions:\")\n        if added:\n            print(f\"  Columns added: {', '.join(sorted(added))}\")\n        if removed:\n            print(f\"  Columns removed: {', '.join(sorted(removed))}\")\n        print(f\"  Column count: {first['count']} (v{first['version']}) -> {last['count']} (v{last['version']})\")\n        issue_detected = True\n    else:\n        print(f\"Schema is stable across {len(schemas)} versions ({last['count']} columns).\")\n        issue_detected = False\nelif len(schemas) == 1:\n    print(f\"Only one schema version found ({schemas[0]['count']} columns). No drift possible.\")\n    issue_detected = False\nelse:\n    print(\"Could not extract schema information from Delta log.\")\n    issue_detected = False\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Schema drift (unexpected column additions or removals) can break downstream consumers. Implement schema validation in your pipelines and use mergeSchema only intentionally.",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL038",
      "category": "Schema",
      "description": "Check for excessively wide tables (more than 100 columns) that can impact scan performance.",
      "pyspark_query": "# Check column count for wide table detection\nschema_info = None\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if 'metadata_schema' in col and isinstance(row[col], str):\n            try:\n                schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                break\n            except:\n                pass\n    if schema_info:\n        break\n\nissue_detected = False\nif schema_info and 'fields' in schema_info:\n    col_count = len(schema_info['fields'])\n    print(f\"Table has {col_count} columns.\")\n    \n    if col_count > 100:\n        print(f\"WARNING: Very wide table ({col_count} columns). This impacts scan performance.\")\n        print(\"Consider vertical partitioning or splitting into related tables.\")\n        issue_detected = True\n    elif col_count > 50:\n        print(f\"NOTE: Table is moderately wide ({col_count} columns). Monitor query performance.\")\n    else:\n        print(\"Column count is within optimal range.\")\nelse:\n    print(\"Could not determine column count from Delta log.\")\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Tables with more than 100 columns suffer from slower scans and higher memory usage. Consider vertical partitioning by splitting logically separate column groups into separate tables.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL039",
      "category": "Maintenance",
      "description": "Monitor table size and flag tables larger than 1TB that require active maintenance strategies.",
      "pyspark_query": "# Monitor total table size\ntotal_size_bytes = 0\nnum_files = 0\n\nfor _, row in df_all.iterrows():\n    for col in row.index:\n        if col.startswith('add_') and '_size' in col:\n            try:\n                size = float(row[col])\n                total_size_bytes += size\n                num_files += 1\n            except:\n                pass\n\ntotal_size_gb = total_size_bytes / (1024 * 1024 * 1024)\ntotal_size_tb = total_size_gb / 1024\n\nif total_size_bytes > 0:\n    if total_size_tb >= 1:\n        print(f\"LARGE TABLE: {total_size_tb:.2f} TB across {num_files} files.\")\n        print(\"Tables of this size require proactive maintenance:\")\n        print(\"  - Regular OPTIMIZE and VACUUM operations\")\n        print(\"  - Proper partitioning and Z-Order/Liquid Clustering\")\n        print(\"  - Monitor query performance closely\")\n        issue_detected = True\n    elif total_size_gb >= 100:\n        print(f\"Table size: {total_size_gb:.1f} GB across {num_files} files.\")\n        print(\"Consider implementing regular maintenance routines.\")\n        issue_detected = False\n    else:\n        print(f\"Table size: {total_size_gb:.1f} GB across {num_files} files. Size is manageable.\")\n        issue_detected = False\nelse:\n    print(\"Could not determine table size from Delta log.\")\n    issue_detected = False\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Tables larger than 1TB require active maintenance strategies including regular OPTIMIZE, VACUUM, proper partitioning/clustering, and performance monitoring.",
      "status": "true",
      "severity": 2,
      "content": "analysis",
      "level": "table"
    },
    {
      "id": "RL040",
      "category": "Structure",
      "description": "Check if Bronze layer tables have an ingestion timestamp column for data lineage tracking.",
      "pyspark_query": "# Check for ingestion timestamp in Bronze layer tables\nlayer = None\nif any(x in lakehouse_name.lower() for x in ['bronze', 'raw', 'landing']):\n    layer = 'bronze'\n\nif layer != 'bronze':\n    print(f\"Table is not in a Bronze layer lakehouse ('{lakehouse_name}'). Skipping ingestion timestamp check.\")\n    issue_detected = False\nelse:\n    schema_info = None\n    for _, row in df_all.iterrows():\n        for col in row.index:\n            if 'metadata_schema' in col and isinstance(row[col], str):\n                try:\n                    schema_info = json.loads(row[col].replace(\"'\", '\"'))\n                    break\n                except:\n                    pass\n        if schema_info:\n            break\n    \n    has_ingestion_col = False\n    if schema_info and 'fields' in schema_info:\n        ingestion_patterns = ['ingestion_time', 'ingestion_date', 'ingestion_timestamp', 'load_date', 'load_time', 'load_timestamp', 'loaded_at', 'ingested_at', 'created_at', '_ingest_time']\n        for field in schema_info['fields']:\n            if 'name' in field:\n                if field['name'].lower() in ingestion_patterns:\n                    has_ingestion_col = True\n                    print(f\"Bronze layer table has ingestion tracking column: '{field['name']}'\")\n                    break\n        \n        if not has_ingestion_col:\n            print(f\"Bronze layer table '{table_name}' is MISSING an ingestion timestamp column.\")\n            print(\"Ingestion timestamps are essential for data lineage in the Medallion Architecture.\")\n            print(f\"Consider adding one of: {', '.join(ingestion_patterns[:5])}\")\n            issue_detected = True\n        else:\n            issue_detected = False\n    else:\n        print(\"Could not extract schema to check for ingestion timestamp column.\")\n        issue_detected = False\n\nprint(\"\")\nprint(\"Indicator: \" + (\"Anomaly - ERR_1001\" if issue_detected else \"Optimized - OPT_2001\"))",
      "recommendation": "Add an ingestion timestamp column (e.g., ingestion_time, load_date) to Bronze layer tables. This is essential for data lineage, debugging pipelines, and implementing incremental loads in the Medallion Architecture.",
      "remediation_template": "ALTER TABLE {full_table_name} ADD COLUMNS (ingestion_timestamp TIMESTAMP)",
      "status": "true",
      "severity": 3,
      "content": "analysis",
      "level": "column"
    }
  ]
}